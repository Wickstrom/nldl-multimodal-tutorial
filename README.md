# nldl-multimodal-tutorial
Practical component of NLDL tutorial on multimodal learning

## Overview

This repository contains practical tutorials for learning about multimodal deep learning, covering the integration of multiple data modalities such as text, images, and more.

## Notebooks

This repository includes three Google Colab notebooks:

1. **Introduction to Multimodal Learning** - Get started with the basics of multimodal learning
   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wickstrom/nldl-multimodal-tutorial/blob/main/1_introduction_to_multimodal_learning.ipynb)

2. **Visual and Text Processing** - Learn about processing techniques for images and text
   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wickstrom/nldl-multimodal-tutorial/blob/main/2_visual_and_text_processing.ipynb)

3. **Advanced Multimodal Techniques** - Explore advanced fusion methods and attention mechanisms
   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wickstrom/nldl-multimodal-tutorial/blob/main/3_advanced_multimodal_techniques.ipynb)

## Data

The `data/` folder contains sample datasets and resources for use with the tutorial notebooks. See the [data README](data/README.md) for more information.

## Getting Started

Click on any of the "Open in Colab" badges above to launch the notebooks directly in Google Colab. No local setup required!

## Topics Covered

- Multimodal learning fundamentals
- Text processing with transformers (BERT)
- Image processing with CNNs (ResNet)
- Cross-modal attention mechanisms
- Feature fusion techniques
- Practical applications (image captioning, VQA, etc.)

## Requirements

The notebooks install all required dependencies automatically when run in Google Colab. Main libraries used:
- PyTorch
- Transformers (Hugging Face)
- TorchVision
- PIL/Pillow
- NumPy
