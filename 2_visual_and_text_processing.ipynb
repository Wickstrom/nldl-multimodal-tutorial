{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Visual and Text Processing\n",
    "\n",
    "This notebook explores techniques for processing visual and textual data, including feature extraction and representation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_setup"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_text"
   },
   "source": [
    "## Text Processing with BERT\n",
    "\n",
    "We'll use BERT to extract features from text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text_processing"
   },
   "outputs": [],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example of multimodal learning.\"\n",
    "\n",
    "# Tokenize and encode\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**inputs)\n",
    "    text_features = outputs.last_hidden_state\n",
    "\n",
    "print(f\"Text features shape: {text_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_vision"
   },
   "source": [
    "## Visual Processing with ResNet\n",
    "\n",
    "We'll use a pre-trained ResNet model to extract visual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visual_processing"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet.eval()\n",
    "\n",
    "# Remove the final classification layer to get features\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create a sample image\n",
    "img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "img_tensor = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    image_features = feature_extractor(img_tensor)\n",
    "\n",
    "print(f\"Image features shape: {image_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_fusion"
   },
   "source": [
    "## Feature Fusion\n",
    "\n",
    "Combining visual and text features is a key step in multimodal learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fusion"
   },
   "outputs": [],
   "source": [
    "# Simple concatenation fusion\n",
    "# Note: In practice, you'd need to handle dimensions more carefully\n",
    "print(\"Visual and text features can be combined using:\")\n",
    "print(\"- Concatenation\")\n",
    "print(\"- Attention mechanisms\")\n",
    "print(\"- Cross-modal transformers\")\n",
    "print(\"- Bilinear pooling\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2_visual_and_text_processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
