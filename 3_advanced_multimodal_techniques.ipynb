{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Advanced Multimodal Techniques\n",
    "\n",
    "This notebook covers advanced techniques in multimodal learning, including attention mechanisms, cross-modal transformers, and practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_setup"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_attention"
   },
   "source": [
    "## Cross-Modal Attention\n",
    "\n",
    "Attention mechanisms allow models to focus on relevant parts of one modality based on information from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "attention_class"
   },
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.scale = dim ** -0.5\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # x1: modality 1 features\n",
    "        # x2: modality 2 features\n",
    "        q = self.query(x1)\n",
    "        k = self.key(x2)\n",
    "        v = self.value(x2)\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "attention = CrossModalAttention(dim=256)\n",
    "x1 = torch.randn(1, 10, 256)  # Text features\n",
    "x2 = torch.randn(1, 49, 256)  # Image features\n",
    "output = attention(x1, x2)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_fusion"
   },
   "source": [
    "## Multimodal Fusion Network\n",
    "\n",
    "A complete network for fusing multiple modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fusion_network"
   },
   "outputs": [],
   "source": [
    "class MultimodalFusionNetwork(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
    "        super(MultimodalFusionNetwork, self).__init__()\n",
    "        \n",
    "        # Project modalities to same dimension\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_proj = nn.Linear(image_dim, hidden_dim)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.attention = CrossModalAttention(hidden_dim)\n",
    "        \n",
    "        # Fusion and classification\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_features, image_features):\n",
    "        # Project to common space\n",
    "        text_proj = self.text_proj(text_features)\n",
    "        image_proj = self.image_proj(image_features)\n",
    "        \n",
    "        # Apply cross-modal attention\n",
    "        attended_features = self.attention(text_proj, image_proj)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat([text_proj, attended_features], dim=-1)\n",
    "        output = self.fusion(combined.mean(dim=1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example\n",
    "model = MultimodalFusionNetwork(\n",
    "    text_dim=768,\n",
    "    image_dim=2048,\n",
    "    hidden_dim=256,\n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "text_feat = torch.randn(1, 20, 768)\n",
    "image_feat = torch.randn(1, 49, 2048)\n",
    "output = model(text_feat, image_feat)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_applications"
   },
   "source": [
    "## Applications\n",
    "\n",
    "Multimodal learning is used in:\n",
    "- **Image Captioning**: Generating text descriptions of images\n",
    "- **Visual Question Answering**: Answering questions about images\n",
    "- **Video Understanding**: Analyzing video content with audio and text\n",
    "- **Cross-modal Retrieval**: Finding images from text queries or vice versa\n",
    "- **Multimodal Sentiment Analysis**: Analyzing sentiment from text, audio, and video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_conclusion"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has covered:\n",
    "1. Introduction to multimodal learning concepts\n",
    "2. Processing techniques for visual and text data\n",
    "3. Advanced fusion and attention mechanisms\n",
    "\n",
    "For more information, check out the data folder for sample datasets and additional resources."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "3_advanced_multimodal_techniques.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
